est
se_est <- as.matrix(summary(fit_sim)$coefficients[8,2]^2)
se_est
res <- bain(x=est, Sigma=se_est, hypothesis="est<0;est=0", n=nrow(dat))
bf_10 <- res[["BFmatrix"]][1,2]
row <- which(rownames(summary(fit_sim)$coefficients) == "A2:B2:C2")
row
summary(fit_sim)$coefficients
row <- which(rownames(summary(fit_sim)$coefficients) == "A:B:C")
row
length(row) == 1
accept[i] <- (bf>10)
(bf>10)
accept[i] <- (bf_10>10)
(bf_10>10)
set.seed(123)
nsim <- 2000
accept <- logical(nsim)
## helper to simulate and test once
for (i in seq_len(nsim)) {
# simulate response under the fitted model m0
simdat <- dat_long
simdat$y <- simulate(m0, re.form = NULL)[[1]]   # simulate full response (including random effects)
# fit model to simulated data
# fit_sim <- anovaBF(y ~ A * B * C + id,
#                     data = as.data.frame(dat_long),
#                     whichRandom = "id")
fit_sim <- try(lmer(y ~ A * B * C + (1|id), data = simdat), silent = TRUE)
if (inherits(fit_sim, "try-error")) {
accept[i] <- NA
next
}
est <- summary(fit_sim)$coefficients[8,1]
names(est) <- "est"
se_est <- as.matrix(summary(fit_sim)$coefficients[8,2]^2)
res <- bain(x=est, Sigma=se_est, hypothesis="est<0;est=0", n=nrow(dat))
bf_10 <- res[["BFmatrix"]][1,2]
accept[i] <- (bf_10>10)
print(i/nsim)
}
accept
# estimate power
accept[accept==TRUE]
# estimate power
accept[accept==TRUE] / length(accept)
# estimate power
length(accept[accept==TRUE]) / length(accept)
res
View(res)
res[["fit"]][["BF.c"]]
# bf_10 <- res[["BFmatrix"]][1,2]
bf_10 <- res[["fit"]][["BF.c"]][1]
bf_10
set.seed(123)
nsim <- 2000
accept <- logical(nsim)
## helper to simulate and test once
for (i in seq_len(nsim)) {
# simulate response under the fitted model m0
simdat <- dat_long
simdat$y <- simulate(m0, re.form = NULL)[[1]]   # simulate full response (including random effects)
fit_sim <- try(lmer(y ~ A * B * C + (1|id), data = simdat), silent = TRUE)
if (inherits(fit_sim, "try-error")) {
accept[i] <- NA
next
}
est <- summary(fit_sim)$coefficients[8,1]
names(est) <- "est"
se_est <- as.matrix(summary(fit_sim)$coefficients[8,2]^2)
res <- bain(x=est, Sigma=se_est, hypothesis="est<0;est=0", n=nrow(dat))
# bf_10 <- res[["BFmatrix"]][1,2]
bf_10 <- res[["fit"]][["BF.c"]][1]
accept[i] <- (bf_10>10)
print(i/nsim)
}
# estimate power
length(accept[accept==TRUE]) / length(accept) # 0.0225
bf <- anovaBF(
y ~ A * B * C + id,
data = dat_long,
whichRandom = "id",
whichModels = "bottom"
)
bf <- anovaBF(
y ~ A * B * C + id,
data = dat_long,
whichRandom = "id"
)
dat_long
bf <- anovaBF(
y ~ A * B * C + id,
data = as.data.frame(dat_long),
whichRandom = "id"
)
# with BayesFactor
dat_long$A <- as.factor(dat_long$A)
dat_long$B <- as.factor(dat_long$B)
dat_long$C <- as.factor(dat_long$C)
bf <- anovaBF(
y ~ A * B * C + id,
data = as.data.frame(dat_long),
whichRandom = "id"
)
?anovaBF
bf <- anovaBF(
y ~ A * B * C + id,
data = as.data.frame(dat_long),
whichRandom = "id",
iterations = 1000
)
bf
View(bf)
64.29454/35.5
bf@bayesFactor[["bf"]]
library(bayestestR)
?bayesfactor_inclusion
library(bayestestR)
fit1 <- lmer(y ~ A * B * C + (1|id), data = dat_long)
fit0 <- lmer(y ~ (1|id), data = dat_long)
fit0
BFmodels <- bayesfactor_models(fit0, fit1, denominator = fit0)
BFmodels
bf_inclusion(BFmodels)
bf_incl <- bf_inclusion(BFmodels)
as.numeric(bf_incl)
fit2 <- lmer(y ~ A * B + (1|id), data = dat_long)
summary(fit2)
fit0 <- lmer(y ~ (1|id), data = dat_long)
fit1 <- lmer(y ~ A + (1|id), data = dat_long)
fit2 <- lmer(y ~ A * B + (1|id), data = dat_long)
fit3 <- lmer(y ~ C + (1|id), data = dat_long)
fit4 <- lmer(y ~ A * C + (1|id), data = dat_long)
fit5 <- lmer(y ~ B * C + (1|id), data = dat_long)
fit_final <- lmer(y ~ A * B * C + (1|id), data = dat_long)
fit0 <- lmer(y ~ (1|id), data = dat_long)
BFmodels <- bayesfactor_models(fit1, fit2, fit3, fit4, fit5, fit_final, denominator = fit0)
bf_incl <- bf_inclusion(BFmodels)
as.numeric(bf_incl)
# reshape to long format
dat_long <- dat %>%
pivot_longer(
cols = -id,
names_to = c("A", "B", "C"),
names_pattern = "A(\\d)B(\\d)C(\\d)",
values_to = "y"
) %>%
mutate(
A = factor(A),
B = factor(B),
C = factor(C),
id = factor(id)
)
fit0 <- lmer(y ~ (1|id), data = dat_long)
fit1 <- lmer(y ~ A + (1|id), data = dat_long)
fit2 <- lmer(y ~ A * B + (1|id), data = dat_long)
fit3 <- lmer(y ~ C + (1|id), data = dat_long)
fit4 <- lmer(y ~ A * C + (1|id), data = dat_long)
fit5 <- lmer(y ~ B * C + (1|id), data = dat_long)
fit_final <- lmer(y ~ A * B * C + (1|id), data = dat_long)
BFmodels <- bayesfactor_models(fit1, fit2, fit3, fit4, fit5, fit_final, denominator = fit0)
bf_incl <- bf_inclusion(BFmodels)
as.numeric(bf_incl)
bf_incl
m0
as.numeric(bf_incl)
fit0 <- lmer(y ~ (1|id), data = dat_long)
fit1 <- lmer(y ~ A + (1|id), data = dat_long)
fit2 <- lmer(y ~ A * B + (1|id), data = dat_long)
fit3 <- lmer(y ~ C + (1|id), data = dat_long)
fit4 <- lmer(y ~ A * C + (1|id), data = dat_long)
fit5 <- lmer(y ~ B * C + (1|id), data = dat_long)
fit_final <- lmer(y ~ A * B * C + (1|id), data = dat_long)
BFmodels <- bayesfactor_models(fit1, fit2, fit3, fit4, fit5, fit_final, denominator = fit0)
bf_incl <- bf_inclusion(BFmodels)
as.numeric(bf_incl)
bf_incl
as.numeric(bf_incl)
bf_incl <- bayesfactor_inclusion(BFmodels)
as.numeric(bf_incl)
log(8)
exp(log(8))
exp(bf_incl)
fit0 <- lmer(y ~ (1|id), data = dat_long)
fit1 <- lmer(y ~ A + (1|id), data = dat_long)
fit2 <- lmer(y ~ A * B + (1|id), data = dat_long)
fit3 <- lmer(y ~ C + (1|id), data = dat_long)
fit4 <- lmer(y ~ A * C + (1|id), data = dat_long)
fit5 <- lmer(y ~ B * C + (1|id), data = dat_long)
fit_final <- lmer(y ~ A * B * C + (1|id), data = dat_long)
BFmodels <- bayesfactor_models(fit1, fit2, fit3, fit4, fit5, fit_final, denominator = fit0)
bf_incl <- bayesfactor_inclusion(BFmodels)
exp(bf_incl)
bf_incl
bfmod <- anovaBF(
y ~ A * B * C + id,
data = as.data.frame(dat_long),
whichRandom = "id",
iterations = 1000
)
bf_incl <- bayesfactor_inclusion(bfmod)
bf_incl
bfmod <- anovaBF(
y ~ A * B * C + id,
data = as.data.frame(dat_long),
whichRandom = "id",
iterations = 1000,
progress=F
)
bf_incl <- bayesfactor_inclusion(bfmod)
exp(bf_incl)
bf_incl
citation("restriktor")
library(ggplot2)
library(gridExtra)
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- FALSE
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .01
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
h0_true <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
h0_false <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
par(mfrow=c(1,2))
h0_true
h0_false
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- TRUE
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .01
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
h0_true <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
h0_false <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
par(mfrow=c(1,2))
h0_true
h0_false
h0_false
h0_true
h0_false
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- F
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .1
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
p.1 <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
p.1
dat
pmps
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- F
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .01
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
p.01 <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
p.01
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- F
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .001
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
p.001 <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
# par(mfrow=c(1,2))
# h0_true
# h0_false
p.001
# sample size
ss <- seq(2, 101)
# sd of the true distribution of the mean
sd_true <- c(.01, .025, .05, .1, 1)
# H0 true or false?
h0 <- T
# if H0 false, then what is the deviation from zero of the true mean (in both directions)
deviation <- .001
# if H0 false, define true means
true_lo <- -deviation
true_hi <- deviation
# make data frame
dat <- expand.grid(ss=ss, sd=sd_true)
# number of iterations
n_iter <- 1000
pmps <- rep(NA, n_iter)
# for all combinations of sample size and sd
for(i in 1:nrow(dat)){
for(j in 1:n_iter){
if(h0==T){ # if H0 true, then truen mean is zero
true_mean <- 0
} else { # otherwise, it deviates from zero in both directions
if(rbinom(1, 1, .5) == 0) {
true_mean <- true_lo
} else{
true_mean <- true_hi
}
}
# draw sample of the mean
x <- rnorm(n=dat$ss[i], mean=true_mean, sd=dat$sd[i])
# calculate PMP for H1: mean < 0
pmps[j] <- pnorm(q=0, mean=mean(x), sd=sd(x))
}
dat$mean_pmp[i] <- mean(pmps) # get the average PMP for each combination of sample size and sd
dat$lo_ci_pmp[i] <- quantile(pmps, probs = .05) # create CIs to show uncertainty
dat$hi_ci_pmp[i] <- quantile(pmps, probs = .95)
print(i/nrow(dat)) # print progress
}
p0 <- ggplot(data=dat, aes(x=ss, y=mean_pmp, color=as.factor(sd)))+
geom_line() +
geom_ribbon(aes(ymin=lo_ci_pmp, ymax=hi_ci_pmp), alpha=.1) +
labs(color="sd", x="N", y="PMP")
# par(mfrow=c(1,2))
# h0_true
# h0_false
p0
p.1
p.001
p.1
par(mfrow=c(2,2))
p0
p.001
p.01
p.1
library(gridExtra)
gridplot(p0, p.001, p.01, p.1)
library(grid)
gridplot(p0, p.001, p.01, p.1)
?gridplot
grid(p0, p.001, p.01, p.1)
grid.arragne(p0, p.001, p.01, p.1)
grid.arrange(p0, p.001, p.01, p.1)
?SSD_longit
